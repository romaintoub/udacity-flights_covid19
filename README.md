# FLIGHTS v. COVID-19 Capstone Project

The pandemic of COVID-19 is striking the world and has a serious impact on the aviation area. In this project, a data lake has been built to show the relations between COVID-19 cases and transportation by air. For example, we could try to find answers to questions such as, how the number of cases in the countries is affecting the number of flights? 

## Goal 
Be able to understand how COVID-19 world cases behave with aviation can be done in so many ways. In this project, the end goal is to build a cleaned and organized schema to facilitate the analysis of the data. The tables created from the different sources are load in S3 with an AWS account. With the COVID-19 world database and Opensky Flights data, an ETL pipeline is built for a data lake hosted on S3. The process is based on 3 important steps: load data from different URLs, process the data into analytics tables using Spark and load them into S3. 

## Data sources
Four important data sources are used to build the data lake. All the source's information are collected in a [csv file](./data/source_url.csv).

### **Covid-19 dataset**
Covid-19 cases dataset is generated by [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data/).
* This table is updated daily and includes data on confirmed cases, deaths, hospitalizations, testing and vaccinations, throughout the duration of the COVID-19 pandemic. 
* This data is already cleaned, we just need to remove some null columns and remove rows for which they are not counts for a country, such as _OWID_SOMETHING_. We can find a lot of columns in this table, this is why we select only the one that reports covid-19 cases and deaths.

Sample record: 
```
{"iso_code": "AFG", "continent": "Asia", "location": "Afghanistan", "date": "2020-02-24", "total_cases": 1, "new_cases": 1, "new_cases_smoothed": null, "total_deaths": null, "new_deaths": null, "new_deaths_smoothed": null, "total_cases_per_million": 0.026, "new_cases_per_million": 0.026, "new_cases_smoothed_per_million": null, "total_deaths_per_million": null, "new_deaths_per_million": null, "new_deaths_smoothed_per_million": null}
```
### **Opensky dataset**
Flights dataset is generated by the [OpenSky Network](https://traffic-viz.github.io/scenarios/covid19.html).
* The data downloaded is derived and cleaned from the full OpenSky dataset to illustrate the development of air traffic during the COVID-19 pandemic. You can find the files [here](https://zenodo.org/record/4485741#.YEAf2C3b1qu).
* It spans all flights seen by the network's more than 2,500 member since 1 January 2019. 
* One of the two most important columns are the airport of origin and the airport of destination of the flight. This is why null values for both columnns are the same time are dropped.

Sample record:
```
 {"callsign": "ETH728", "number": null, "icao24": "040188", "registration": null, "typecode": null, "origin": "KEWR", "destination": "EBBR", "firstseen": "2020-11-30 02:23:10+00:00", "lastseen": "2020-12-01 05:42:23+00:00", "day": "2020-12-01 00:00:00+00:00", "latitude_1": 40.6700832561, "longitude_1": -74.1828086159, "altitude_1": 0.0, "latitude_2": 50.8949890137, "longitude_2": 4.506101866, "altitude_2": 30.48}
```
### **Our airports dataset**
Airports and airlines information are coming from [OurAirports](https://ourairports.com/data/).
* Countries: a list of the world's countries.
* Airports: large file containing information on all the airports around the world. 
* These 2 tables contains a lot of non-alphanumeric values which refers to null entries. Most of the cleaning of the two tables reside in replacing these values by nulls and dropping null and duplicates so that we can join these two tables properly.

Sample record ```countries.csv```:
```
{"id": 302672, "code": "AD", "name": "Andorra", "continent": "EU", "wikipedia_link": "https://en.wikipedia.org/wiki/Andorra", "keywords": null}
```

Sample record ```airports.csv```:
```
{"id": 6523, "ident": "00A", "type": "heliport", "name": "Total Rf Heliport", "latitude_deg": 40.070801, "longitude_deg": -74.933601, "elevation_ft": 11.0, "continent": null, "iso_country": "US", "iso_region": "US-PA", "municipality": "Bensalem", "scheduled_service": "no", "gps_code": "00A", "iata_code": null, "local_code": "00A", "home_link": null, "wikipedia_link": null, "keywords": null}	
```

### **Openflights dataset**
The mapping of the flights are done with [Openflights](https://github.com/jpatokal/openflights).
* Airlines: list of all the airlines around the world.
* This table has also non-alphanumeric values and needs to be cleaned.

Sample record:
```
{"airline_id": 3, "name": "1Time Airline", "alias": "\N", "IATA": "1T", "ICAO": "RNX", "callsign": "NEXTIME", "country": "South Africa", "active": "Y"}
```

## Project Files
* ```dl.cfg```: Config file that contains AWS credentials. You'll need to create this file to run the following script.
* ```source_to_s3.py```: Script that downloads, reads the data in source, transforms the datasets into tables and load them into S3.
* ```notebooks``` folder:
	* ```process_dim_tables.ipynb```: Python notebook that reads the data from source and process the dimension tables, such as countries, airports and airlines.
	* ```process_fact_tables.ipynb```: Python notebook that reads the data from source and process the fact tables, such as covid-19 and flights datasets.
	* ```flights_v_covid_analysis.ipynb```: Python notebook that reads table to S3 to do a primarly analysis of the schema.
* ```data```folder:
	* ```source```folder: Folder containing all the source data, downloaded with ```source_to_s3.py```
	* ```source_url.csv```: This table contains all the information about the dataset used as source.
* ```utils```folder: 
	* ```data_utils.py```: Script containing functions to download source data.

## Schema for Flights v. COVID-19 Analysis 

![Flights and Covid-19 Schema](flights.jpg)
The final schema is a star schema. At the exception that there are 2 fact tables, the first one is the **flights** data that collects every flights per day and the other one is the **covid19** dataset that collects the number of cases per day as well. I chose this data model because it provides many benefits. Since this model has two fact tables and three dimension tables, it provides the final user greater flexibility, allowing them to perform a variety of different queries. Even if combining the two fact tables requires some aggregation with the date and the countries, it necessitates less joins than a normalized transactional schema, and as result, is more efficient and query performance is much better.

## Data Dictionnary 

### Dimension Tables

* ```airlines```

| Column Name | Specifications | Description |
| ------------- | ------------- | ------------- |
| icao | Primary Key | Three-letter ICAO code, unique identifier for this airline. |
| name | | Name of the airline. |
| alias |  | Alias of the airline. For example, All Nippon Airways is commonly known as "ANA". |
| iata | | Two-letter IATA code.
| callsign | | Airline callsign. |
| country | | Country or territory where airport is located. |

* ```airports```

| Column Name | Specifications | Description |
| ------------- | ------------- | ------------- |
| ident | Primary Key | The 4-letter ICAO code if available. Otherwise, it will be a local airport code (if no conflict), or if nothing else is available, an internally-generated code starting with the ISO2 country code, followed by a dash and a four-digit number. |
| type | | The type of the airport. Allowed values are "closed_airport", "heliport", "large_airport", "medium_airport", "seaplane_base", and "small_airport". |
| name | | The official airport name, including "Airport", "Airstrip", etc. |
| latitude_deg | | The airport latitude in decimal degrees (positive for north). | 
| longitude_def | | The airport longitude in decimal degrees (positive for east). | 
| elevation_ft | | The airport elevation MSL in feet. | 
| iso_country | | The two-character ISO 3166:1-alpha2 code for the country where the airport is (primarily) located. | 
| iso_region | | An alphanumeric code for the high-level administrative subdivision of a country where the airport is primarily located (e.g. province, governorate), prefixed by the ISO2 country code and a hyphen. | 
| municipality | | The primary municipality that the airport serves (when available). Note that this is not necessarily the municipality where the airport is physically located. | 
| scheduled_service | | "yes" if the airport currently has scheduled airline service; "no" otherwise. | 
| gps_code | | The code that an aviation GPS database (such as Jeppesen's or Garmin's) would normally use for the airport. This will always be the ICAO code if one exists. | 
| iata_code | | The three-letter IATA code for the airport. | 
| local_code | | The local country code for the airport, if different from the gps_code and iata_code fields (used mainly for US airports). | 

* ```countries```

| Column Name | Specifications | Description |
| ------------- | ------------- | ------------- |
| iso_code2 | Primary Key | The two-character ISO 3166:1-alpha2 code for the country. | 
| iso_code3 | | The three-character ISO 3166:1-alpha3 code for the country. |
| name | | The common English-language name for the country. | 
| iso_continent | | The code for the continent where the country is (primarily) located. | 
| continent | | The name of the continent. | 
| population | | The population of the country in 2020. Source: United Nations, Department of Economic and Social Affairs, Population Division, World Population Prospects 2019 Revision. | 

## Fact Tables 

* ```covid19```

| Column Name | Specifications | Description |
| ------------- | ------------- | ------------- |
| iso_code3 | | The three-character ISO 3166:1-alpha3 code for the country. |
| date | Date YYYY-MM-DD format | Date of observation. | 
| total_cases | | Total confirmed cases of COVID-19. | 
| new_cases | | New confirmed cases of COVID-19. | 
| new_cases_smoothed | | New confirmed cases of COVID-19 with a 7-day smoothed. | 
| total_deaths | | Total deaths attributed to COVID-19. | 
| new_deaths | | New deaths attributed to COVID-19. | 
| new_deaths_smoothed | | New deaths attributes to COVID-19 with a 7-day smoothed. | 

* ```flights```

| Column Name | Specifications | Description |
| ------------- | ------------- | ------------- |
| year | | Year of the UTC day of the last message received by the OpenSky Network. | 
| month | | Month of the UTC day of the last message received by the OpenSky Network. | 
| day | | The UTC day of the last message received by the OpenSky Network. | 
| callsign | | The identifier of the flight displayed on ATC screens. | 
| callsign_icao | | Three-letter ICAO code, unique identifier for this airline. |
| icao24 | | The transponder unique identification number. | 
| registration | | The aircraft tail number. | 
| typecode | | The aircraft model type. | 
| ident_origin | | The four-letter ICAO code for the origin of the airport of the flight. |
| iso_country_origin | | The two-character ISO 3166:1-alpha2 code for the country where the origin airport is located. | 
| ident_destination | | The four-letter ICAO code for the destination of the airport of the flight. | 
| iso_country_destination | | The two-character ISO 3166:1-alpha2 code for the country where the destination airport is located. | 
| firstseen | Timestamp YYYY-MM-DD HH:mm:ss format | The UTC timestamp of the first message received by the OpenSky Network. |
| lastseen | Timestamp YYYY-MM-DD HH:mm:ss format | The UTC timestamp of the last message received by the OpenSky Network. |
| latitude_deg1 | | The first detected position latitude in decimal degrees of the aircraft (positive for north). | 
| longitude_deg1 | | The first detected position longitude in decimal degrees of the aircraft (positive for east). |
| altitude_ft1 | | The first detected position altitude in feets of the aircraft. |
| latitude_deg2 | | The last detected position latitude in decimal degrees of the aircraft (positive for north). | 
| longitude_deg2 | | The last detected position longitude in decimal degrees of the aircraft (positive for east). |
| altitude_ft2 | | The last detected position altitude in feets of the aircraft. |

## Step in Process

1. Start a Spark Session 
2. Clean raw data, create dimension tables, quality check and save to parquet: 
	* ```airlines```
		* Download airlines dataset from url in ```source_url.csv```.
		* Load airlines csv file with Spark.
		* Use schema structure to cast column types.
		* Check if _icao_ and _iata_ columns are alpha-numeric or set the values to null.
		* Drop null values and duplicates in _icao_ column to create a primary key.
		* Create the table with relevant columns.
		* Run quality check.
		* Save to parquet for downstream query.
	* ```airports```
		* Download airports dataset from url in ```source_url.csv```.
		* Load airports csv file with Spark.
		* Use schema structure to cast column types.
		* Cast _scheduled_service_ to Boolean by replacing 'yes' values by True.
		* Drop null values and duplicates in _ident_ column to create a primary key.
		* Create first table with relevant columns.
		* Create a second smaller table with only _ident_ and _iso_country_ column.
		* Write the new table to staging S3.
		* Run quality check on both tables.
		* Save to parquet for downstream query.
	* ```countries```
		* Download countries dataset from url in ```source_url.csv```.
		* Load countries csv file with Spark.
		* Use schema structure to cast column types.
		* Drop null values and duplicates in _iso_code2_ column to create a primary key.
		* Download covid19 dataset from url in ```source_url.csv```.
		* Load covid19.csv file to collect _iso_code3_ with _location_ key.
		* Left join countries with covid19 on the name of the country. 
		* Select the right columns to create the table.
		* Run quality check.
		* Save to parquet for downstream query.
	* ```covid19```
		* Load covid19.csv file in source with Spark.
		* Drop duplicates in _iso_code_ and _date_ columns
		* Check if _iso_code3_ is alpha-numeric or set the values to null.
		* Drop null values in _iso_code3_ and _location_ columns.
		* Convert _date_ column to Date type and numeric columns to float type.
		* Select the relevant columns to create the table.
		* Run quality check.
		* Save to parquet for downstream query.
	* ```flights```
		* Download all the flightlist dataset from urls in ```source_url.csv```. Requires around 4GB storage (and a little bit time to download).
		* Load all the flight files with Spark.
		* Use schema structure to cast column types.
		* Drop null values in _day_ column.
		* Drop rows if both _origin_ and _destination_ columns are null (for the following join).
		* Convert _day_ column to date and add new columns _month_ and _year_ from this first key.
		* Drop duplicates.
		* Load airports.csv 
		* Join flights with airport codes on the airport code of origin and destination to add the country of origin and destination of the aircraft.
		* Select the relevant columns to create the table.
		* Run quality check.
		* Save to parquet for downstream query.

## Basis of the data structure and future directions

For this project, I decided to use Spark because it allows us to manipulate and transform big data and write the outputted structure to sql, json, or parquet files. The final schema is a star schema, because this allows all users to make more efficient data queries by necessitating less joins. Going forward this data can be done in batch processing, daily, weekly, or monthly intervals. The decision can be made based on the availability of the original data, but more importantly based on the needs of the final customer and the questions being asked. If we are asking questions like the questions posed above (in Scope and Data), weekly, or even monthly would be fine. Moving forward, this structure could be altered and re-structured based on the needs of the final customer. The following are examples:

**What if the data was increased by 100x?** If the data was increaed by 100x, we could keep the same format, potentially increase the frequency of transformation. The most important thing we could do is spin up larger EC2 instances using Spark. This would give us greater capacity to handle the increased volume of data.

**What if the data populates a dashboard that must be updated on a daily basis by 7am every day?** In order to account for this outcome, we could schedule DAGs (for example, using Airflow) that would run every morning at 5am, to ensure that the data would be ready for the customer to view at 7am. It would be important to schedule warnings and to monitor to ensure that if the DAG was interrupted, we could investigate before final customers are effected.

**What if the database needed to be accessed by 100+ people?** We could either scale up the number of nodes/workers in our cluster, we could implement concurrent scaling in Redshift, or multi-cluster warehousing (in Snowflake), or we could potentially consider using a no-sql database like Apache Cassandra, where our schema conforms to the specified queries our customers need.

## Environment
* Python 3.6 or above.
* Apache Spark 3.0 or above.
* AWS EMR Cluster, S3 buckets access.

## How to Run
1. Install requirements above. Creating an environment with Pyspark and Pandas is strongly recommanded (see _udacity-env_)

2. Create ```dl.cfg``` with your AWS IAM Credentials.

File format for **dl.cfg**
```
$ cat > dl.cfg
[AWS]
AWS_ACCESS_KEY_ID=<your_access_key>
AWS_SECRET_ACCESS_KEY=<your_secret_access>
```

3. Create a bucket in AWS and specify its path with _output_data_ in ```source_to_s3.py```.

4. Run etl.py
```
$ python source_to_s3.py
```

## License
* Hasell, J., Mathieu, E., Beltekian, D. _et al._ A cross-country database of COVID-19 testing. _Sci Data_ **7**, 345 (2020). [DOI/10.1038/s41597-020-00688-8](https://doi.org/10.1038/s41597-020-00688-8)
* Matthias Sch√§fer, Martin Strohmeier, Vincent Lenders, Ivan Martinovic and Matthias Wilhelm.
"Bringing Up OpenSky: A Large-scale ADS-B Sensor Network for Research".
In Proceedings of the 13th IEEE/ACM International Symposium on Information Processing in Sensor Networks (IPSN), pages 83-94, April 2014. [IEEE/10.1109/IPSN.2014.6846743](https://ieeexplore.ieee.org/document/6846743)
* Xavier Olive. "traffic, a toolbox for processing and analysing air traffic data." Journal of Open Source Software 4(39), July 2019.